{
    "cells": [
     {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Overwriting src/data_setup.py\n"
        ]
       }
      ],
      "source": [
       "%%writefile src/data_setup.py\n",
       "\"\"\"\n",
       "todo\n",
       "\"\"\"\n",
       "import os\n",
       "import numpy as onp\n",
       "import jax\n",
       "import jax.numpy as jnp\n",
       "\n",
       "\n",
       "def create_dataloaders(\n",
       "    train_dir: str, \n",
       "    train_labels_dir: str, \n",
       "    test_dir: str, \n",
       "    test_labels_dir: str, \n",
       "    batch_size: int, \n",
       "    ):\n",
       "    \"\"\"Creates training and testing DataLoaders.\n",
       "\n",
       "\n",
       "    Args:\n",
       "    train_dir: Path to training directory.\n",
       "    test_dir: Path to testing directory.\n",
       "\n",
       "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
       "\n",
       "\n",
       "    Returns:\n",
       "\n",
       "    \"\"\"\n",
       "    # Use ImageFolder to create dataset(s)\n",
       "    train_data = onp.load(train_dir)\n",
       "    test_data = onp.load(test_dir)\n",
       "\n",
       "    # Get class names\n",
       "    train_labels = onp.load(train_labels_dir)\n",
       "    test_labels = onp.load(test_labels_dir)\n",
       "\n",
       "    train_labels = jnp.squeeze(jax.nn.one_hot((train_labels), num_classes=10))\n",
       "    test_labels = jnp.squeeze(jax.nn.one_hot((test_labels), num_classes=10))\n",
       "\n",
       "\n",
       "    # Turn data into data loaders\n",
       "    num_train = train_data.shape[0]\n",
       "    num_complete_batches, leftover = divmod(num_train, batch_size)\n",
       "    num_batches = num_complete_batches + bool(leftover)\n",
       "\n",
       "    def train_data_stream():\n",
       "        rng = npr.RandomState(0)\n",
       "        while True:\n",
       "            perm = rng.permutation(num_train)\n",
       "            for i in range(num_batches):\n",
       "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
       "            yield train_images[batch_idx], train_labels[batch_idx]\n",
       "        return train_data_stream    \n",
       "    train_dataloader = train_data_stream()\n",
       "\n",
       "    def valid_data_stream(): #todo batch?\n",
       "        while True:\n",
       "            yield test_data, test_labels\n",
       "        return valid_data_stream\n",
       "    test_dataloader = valid_data_stream\n",
       "\n",
       "    return train_dataloader, test_dataloader, train_labels"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
       "# !pip install -U jax jaxlib\n",
       "# import jax"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Overwriting src/model_builder.py\n"
        ]
       }
      ],
      "source": [
       "%%writefile src/model_builder.py\n",
       "\"\"\"\n",
       "Contains JAX (stax) model code to instantiate an MLP model.\n",
       "\"\"\"\n",
       "import jax\n",
       "from jax import numpy as np\n",
       "from jax import random\n",
       "\n",
       "import neural_tangents as nt\n",
       "from neural_tangents import stax\n",
       "\n",
       "\n",
       "def MLP_stax(input_shape, hidden_units, output_shape, batch_size):\n",
       "    nn_init, nn_apply, _ = stax.serial(\n",
       "                            stax.Dense(hidden_size),\n",
       "                            stax.Relu(),\n",
       "                            stax.Dense(output_shape)\n",
       "                            )\n",
       "\n",
       "    rng = random.PRNGKey(0)\n",
       "    in_shape = (batch_size,) + input_shape\n",
       "    out_shape, params = init_fun(rng, in_shape)\n",
       "\n",
       "    assert out_shape == (batch_size, output_shape), f\"Output shape is {out_shape}, but should be {(batch_size, output_shape)}\"\n",
       "    \n",
       "    return nn_init, nn_apply"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Overwriting src/jax_extras.py\n"
        ]
       }
      ],
      "source": [
       "%%writefile src/jax_extras.py\n",
       "\n",
       "@jit\n",
       "def train_step(step, opt_state,  batch_data, loss_fn):\n",
       "    \"\"\"Implements train step.\n",
       "    \n",
       "    Args:\n",
       "        step: Integer representing the step index\n",
       "        opt_state: Current state of the optimizer\n",
       "        batch_data: A batch of data (images and labels)\n",
       "    Returns:\n",
       "        Batch loss, batch accuracy, updated optimizer state\n",
       "    \"\"\"\n",
       "    params = get_params(opt_state)\n",
       "    batch_loss, batch_gradients = value_and_grad(loss_fn)(params, batch_data)\n",
       "    batch_accuracy = calculate_accuracy(params, batch_data)\n",
       "    return batch_loss, batch_accuracy, opt_update(step, batch_gradients, opt_state)\n",
       "\n",
       "@jit\n",
       "def test_step(opt_state, batch_data):\n",
       "    \"\"\"Implements train step.\n",
       "\n",
       "    Args:\n",
       "        opt_state: Current state of the optimizer\n",
       "        batch_data: A batch of data (images and labels)\n",
       "    Returns:\n",
       "        Batch loss, batch accuracy\n",
       "    \"\"\"\n",
       "    params = get_params(opt_state)\n",
       "    batch_loss = loss_fn(params, batch_data)\n",
       "    batch_accuracy = calculate_accuracy(params, batch_data)\n",
       "    return batch_loss, batch_accuracy\n",
       "\n",
       "\n",
       "def calculate_accuracy(params, batch_data):\n",
       "    \"\"\"Implements accuracy metric.\n",
       "    \n",
       "    Args:\n",
       "        params: Parameters of the network\n",
       "        batch_data: A batch of data (images and labels)\n",
       "    Returns:\n",
       "        Accuracy for the current batch\n",
       "    \"\"\"\n",
       "    inputs, targets = batch_data\n",
       "    target_class = jnp.argmax(targets, axis=1)\n",
       "    predicted_class = jnp.argmax(nn_apply(params, inputs), axis=1)\n",
       "    return jnp.mean(predicted_class == target_class)\n",
       "\n",
       "def cross_loss_fn(params, batch_data):\n",
       "    \"\"\"Implements cross-entropy loss function.\n",
       "    \n",
       "    Args:\n",
       "        params: Parameters of the network\n",
       "        batch_data: A batch of data (images and labels)\n",
       "    Returns:\n",
       "        Loss calculated for the current batch\n",
       "    \"\"\"\n",
       "    inputs, targets = batch_data\n",
       "    preds = nn_apply(params, inputs)\n",
       "    return -jnp.mean(jnp.sum(log_softmax(preds) * targets, axis=1))\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Overwriting src/engine.py\n"
        ]
       }
      ],
      "source": [
       "%%writefile src/engine.py\n",
       "\"\"\"\n",
       "Contains functions for training and testing a JAX model.\n",
       "\"\"\"\n",
       "\n",
       "from tqdm.auto import tqdm\n",
       "from typing import Dict, List, Tuple\n",
       "\n",
       "\n",
       "def JAX_train_step(model: callable, # could be also \"params: Tuple\"\n",
       "               loss_fn: callable, \n",
       "               optimizer: callable,\n",
       "               dataloader,\n",
       "               opt_state) -> Tuple:\n",
       "  \"\"\"Trains a JAX model for a single epoch.\n",
       "\n",
       "    ...........\n",
       "\n",
       "  Args:\n",
       "    model: \n",
       "    dataloader: \n",
       "    loss_fn: \n",
       "    optimizer: \n",
       "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
       "\n",
       "  Returns:\n",
       "\n",
       "  \"\"\"\n",
       "\n",
       "\n",
       "  # Setup train loss and train accuracy values\n",
       "  train_loss, train_acc = 0, 0\n",
       "\n",
       "  #opt_init, opt_update, get_params = optimizers.adam(step_size=learning_r)\n",
       "\n",
       "  # Loop through data loader data batches\n",
       "  for batch, (X, y) in enumerate(dataloader):\n",
       "\n",
       "      # Forward pass\n",
       "      y_pred = model(X)\n",
       "\n",
       "\n",
       "      loss_value, acc, opt_state = train_step(step, opt_state, batch, loss_fn)\n",
       "\n",
       "      train_loss += loss #.item()\n",
       "      train_acc += acc\n",
       "\n",
       "\n",
       "  # Adjust metrics to get average loss and accuracy per batch \n",
       "  train_loss = train_loss / len(dataloader)\n",
       "  train_acc = train_acc / len(dataloader)\n",
       "  return train_loss, train_acc\n",
       "\n",
       "\n",
       "def JAX_test_step(model: callable, # could be also \"params: Tuple\"\n",
       "              loss_fn: callable, \n",
       "              dataloader) -> Tuple:\n",
       "  test_loss, test_acc = 0, 0\n",
       "  for batch, (X, y) in enumerate(dataloader):\n",
       "\n",
       "    test_loss_value, test_acc = test_step(opt_state, batch)\n",
       "\n",
       "    test_loss += test_loss_value\n",
       "    test_acc += test_acc\n",
       "\n",
       "  # Adjust metrics to get average loss and accuracy per batch \n",
       "  test_loss = test_loss / len(dataloader)\n",
       "  test_acc = test_acc / len(dataloader)\n",
       "  return test_loss, test_acc\n",
       "\n",
       "def train(model: callable, \n",
       "          optimizer: callable,\n",
       "          loss_fn: callable,\n",
       "          epochs: int,\n",
       "          train_dataloader,\n",
       "          test_dataloader \n",
       "          ) -> Dict[str, List]:\n",
       "\n",
       "    \n",
       "  \"\"\"T\n",
       "\n",
       "  Args:\n",
       "\n",
       "  Returns:\n",
       " \n",
       "  \"\"\"\n",
       "  # Create empty results dictionary\n",
       "  results = {\"train_loss\": [],\n",
       "      \"train_acc\": [],\n",
       "      \"test_loss\": [],\n",
       "      \"test_acc\": []\n",
       "  }\n",
       "\n",
       "  # Loop through training and testing steps for a number of epochs\n",
       "  for epoch in tqdm(range(epochs)):\n",
       "      train_loss, train_acc = JAX_train_step(model=model,\n",
       "                                          dataloader=train_dataloader,\n",
       "                                          loss_fn=loss_fn,\n",
       "                                          optimizer=optimizer\n",
       "                                          )\n",
       "      test_loss, test_acc = JAX_test_step(model=model,\n",
       "          dataloader=test_dataloader,\n",
       "          loss_fn=loss_fn\n",
       "          )\n",
       "\n",
       "      # Print out what's happening\n",
       "      print(\n",
       "          f\"Epoch: {epoch+1} | \"\n",
       "          f\"train_loss: {train_loss:.4f} | \"\n",
       "          f\"train_acc: {train_acc:.4f} | \"\n",
       "          f\"test_loss: {test_loss:.4f} | \"\n",
       "          f\"test_acc: {test_acc:.4f}\"\n",
       "      )\n",
       "\n",
       "      # Update results dictionary\n",
       "      results[\"train_loss\"].append(train_loss)\n",
       "      results[\"train_acc\"].append(train_acc)\n",
       "      results[\"test_loss\"].append(test_loss)\n",
       "      results[\"test_acc\"].append(test_acc)\n",
       "\n",
       "  # Return the filled results at the end of the epochs\n",
       "  return results"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Overwriting src/utils.py\n"
        ]
       }
      ],
      "source": [
       "%%writefile src/utils.py\n",
       "\"\"\"\n",
       "Contains various utility functions for PyTorch model training and saving.\n",
       "\"\"\"\n",
       "from flax import serialization\n",
       "from pathlib import Path\n",
       "import os\n",
       "\n",
       "def save_model(model_params,\n",
       "               target_dir: str,\n",
       "               model_name: str):\n",
       "    \"\"\"Saves JAX model parameters to a file in a target directory.\n",
       "\n",
       "        Args:\n",
       "        model_params: JAX model parameters to save.\n",
       "        target_dir: A directory for saving the model to.\n",
       "        model_name: A filename for the saved model. Should include\n",
       "            either \".pkl\" or \".params\" as the file extension.\n",
       "\n",
       "        Example usage:\n",
       "        save_model(model_params, \"models\", \"model_name.pkl\")\n",
       "    \"\"\"\n",
       "    # Create target directory\n",
       "    target_dir_path = Path(target_dir)\n",
       "    target_dir_path.mkdir(parents=True,\n",
       "                            exist_ok=True)\n",
       "\n",
       "    # Create model save path\n",
       "    assert model_name.endswith(\".pkl\") or model_name.endswith(\".params\"), \"model_name should end with '.pkl' or '.params'\"\n",
       "    model_save_path = target_dir_path / model_name\n",
       "\n",
       "    # Save the model state_dict()\n",
       "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
       "    with open(model_save_path, \"wb\") as f:\n",
       "        serialization.to_bytes(model_params, f)\n",
       "\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Overwriting src/train.py\n"
        ]
       }
      ],
      "source": [
       "%%writefile src/train.py\n",
       "\"\"\"\n",
       "Trains a PyTorch image classification model using device-agnostic code.\n",
       "\"\"\"\n",
       "\n",
       "import os\n",
       "import torch\n",
       "import data_setup, engine, model_builder, utils\n",
       "from src.jax_extras import cross_loss_fn\n",
       "\n",
       "from torchvision import transforms\n",
       "\n",
       "# Setup hyperparameters\n",
       "NUM_EPOCHS = 5\n",
       "BATCH_SIZE = 32\n",
       "HIDDEN_UNITS = 10\n",
       "LEARNING_RATE = 0.001\n",
       "INPUT_SHAPE = 40\n",
       "\n",
       "# Setup directories\n",
       "train_dir = \"datasets/X_train.npy\"\n",
       "train_labels_dir = \"datasets/y_train.npy\"\n",
       "test_dir = \"datasets/X_test.npy\"\n",
       "test_labels_dir = \"datasets/y_test.npy\"\n",
       "\n",
       "\n",
       "\n",
       "# Create DataLoaders with help from data_setup.py\n",
       "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
       "    train_dir=train_dir,\n",
       "    train_labels_dir=train_labels_dir,\n",
       "    test_dir=test_dir,\n",
       "    test_labels_dir=test_labels_dir,\n",
       "    batch_size=BATCH_SIZE\n",
       ")\n",
       "\n",
       "# Create model with help from model_builder.py\n",
       "nn_init, model = model_builder.MLP_stax(\n",
       "    input_shape=INPUT_SHAPE,\n",
       "    hidden_units=HIDDEN_UNITS,\n",
       "    output_shape=len(class_names),\n",
       "    batch_size = BATCH_SIZE\n",
       ")\n",
       "\n",
       "# Set loss and optimizer\n",
       "loss_fn = cross_loss_fn\n",
       "optimizer = opt_init, opt_update, get_params = optimizers.momentum(LEARNING_RATE, 0.9)\n",
       "\n",
       "# Start training with help from engine.py\n",
       "engine.train(model=model,\n",
       "             train_dataloader=train_dataloader,\n",
       "             test_dataloader=test_dataloader,\n",
       "             loss_fn=loss_fn,\n",
       "             optimizer=optimizer,\n",
       "             epochs=NUM_EPOCHS\n",
       "             )\n",
       "\n",
       "# Save the model with help from utils.py\n",
       "utils.save_model(model=model,\n",
       "                 target_dir=\"models\",\n",
       "                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "  # Initialize model\n",
       "  opt_init, opt_update, get_params = optimizer"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
     },
     "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }